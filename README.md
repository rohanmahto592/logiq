ğŸ“˜ LogIQ â€” Intelligent Log Scanner, Analyzer & Parquet Pipeline

LogIQ is a lightweight, high-performance log processing engine built in Go + Python.
It automatically scans log directories, extracts events, converts logs into Parquet, detects anomalies, and generates JSON + HTML reports â€” all on a scheduled interval.

Designed for developers, SREs, and observability systems who want a simple, blazing-fast, local-first log intelligence tool.

ğŸš€ Features
ğŸ” 1. Smart Log Scanner (Go)

Scans multiple log files using patterns (*.log, nested folders)

Includes/excludes lines using regex-based filters

Extracts timestamps using multiple regex formats

Handles multi-format logs: JSON logs, flat logs, mixed logs

ğŸ“¦ 2. JSON â†’ Parquet Conversion (Go)

Logs saved as structured JSON arrays

Parquet conversion using columnar schema

Automatic folder partitioning:

parquet/year=YYYY/month=MM/day=DD/*.parquet

ğŸ§  3. Intelligent Log Analyzer (Go)

Detects anomalies such as:

ERROR spikes

Critical events

Timeouts

Pattern-based anomaly rules (alert_rules)

ğŸ“Š 4. Reporting Engine (Go + Python)

Generates JSON report

Generates clean HTML report

Stores reports inside /reports/

â± 5. Scheduler

Runs automatically every interval_seconds from logiq.yaml.

ğŸ 6. Python Query Engine

Python module (queryengine/engine.py) allows:

DuckDB querying of parquet logs

Aggregations, filtering, dashboards

Used by Streamlit UI (if needed)

ğŸ“ Folder Structure
LOGIQ/
â”‚
â”œâ”€â”€ cmd/logiq/
â”‚   â””â”€â”€ main.go                   # Main runner + scheduler
â”‚
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ logiq.yaml                # Central configuration file
â”‚
â”œâ”€â”€ logs/
â”‚   â”œâ”€â”€ temp.log                  # Sample logs
â”‚   â””â”€â”€ writex.log
â”‚
â”œâ”€â”€ jsonLogs/
â”‚   â””â”€â”€ YYYY-MM-DD_logs.json      # Raw logs saved as JSON
â”‚
â”œâ”€â”€ mnt/data/logiq/parquet/
â”‚   â””â”€â”€ year=2025/month=11/day=20/
â”‚       â”œâ”€â”€ logs_*.parquet
â”‚
â”œâ”€â”€ pkg/
â”‚   â”œâ”€â”€ analyzer/                 # Pattern-based anomaly detection
â”‚   â”‚   â””â”€â”€ analyzer.go
â”‚   â”œâ”€â”€ config/                   # YAML loader + struct bindings
â”‚   â”‚   â””â”€â”€ config.go
â”‚   â”œâ”€â”€ jsonConvertor/            # Saves logs as JSON arrays
â”‚   â”‚   â””â”€â”€ convertor.go
â”‚   â”œâ”€â”€ parquetwriter/            # JSON â†’ Parquet writer
â”‚   â”‚   â””â”€â”€ parquetwriter.go
â”‚   â”œâ”€â”€ reporter/                 # HTML + JSON report generator
â”‚   â”‚   â””â”€â”€ reporter.go
â”‚   â””â”€â”€ scanner/                  # Log scanner
â”‚       â””â”€â”€ scanner.go
â”‚
â”œâ”€â”€ queryengine/                  # Python DuckDB engine
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ engine.py
â”‚
â”œâ”€â”€ reports/
â”‚   â”œâ”€â”€ logiq-report.json         # Output JSON report
â”‚   â””â”€â”€ logiq-report.json.html    # HTML report
â”‚
â””â”€â”€ ui/
    â””â”€â”€ main.py                   # (Optional) Streamlit dashboard

âš™ï¸ Configuration (logiq.yaml)
log_paths:
  - "./logs/*.log"

include_patterns:
  - "(?i)ERROR"
  - "(?i)WARN"
  - "(?i)CRITICAL"

exclude_patterns:
  - "(?i)DEBUG"

timestamp_patterns:
  - "\\d{4}-\\d{2}-\\d{2}[ T]\\d{2}:\\d{2}:\\d{2}"
  - "[A-Z][a-z]{2} [0-9]{1,2} [0-9:]{8}"
  - "\\d{2}/[A-Z][a-z]{3}/\\d{4}:\\d{2}:\\d{2}:\\d{2}"

alert_rules:
  - keyword: "(?i)ERROR"
    severity: "high"
  - keyword: "(?i)timeout"
    severity: "medium"

output:
  format: "json,html"
  report_file: "reports/logiq-report.json"

analysis:
  group_by: "keyword"
  threshold_spike: 50
  interval_seconds: 10

output_paths:
  parquet_dir_path: "mnt/data/logiq/parquet/"
  json_dir_path: "jsonLogs/"

â–¶ï¸ Running LogIQ
Manual Run

Scan logs immediately:

go run cmd/logiq/main.go --scan --report

Scheduled Mode (default)

Runs every interval_seconds defined in config:

go run cmd/logiq/main.go

Custom config file
go run cmd/logiq/main.go --config myconfig.yaml

ğŸ§ª Example Query with DuckDB

Inside Python:

from queryengine.engine import ParquetQueryEngine

engine = ParquetQueryEngine()
df = engine.query("""
    SELECT *
    FROM logs
    WHERE level = 'ERROR'
    ORDER BY timestamp DESC
    LIMIT 100
""")

print(df)

ğŸ“„ Sample Output Report
reports/logiq-report.json

Contains:

anomaly counts

event summaries

grouped statistics

reports/logiq-report.json.html

Clean HTML report viewable in browser.
